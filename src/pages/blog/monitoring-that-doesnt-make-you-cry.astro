---
import BlogLayout from "../../layouts/BlogLayout.astro";

const post = {
  title: "Monitoring That Doesn't Make You Cry",
  date: "2024-12-08",
  readTime: "4 min read",
  category: "DevOps",
  excerpt:
    "How to set up observability that actually helps your team sleep better at night. No complex dashboards required.",
  slug: "monitoring-that-doesnt-make-you-cry"
};
---

<BlogLayout post={post}>
  <h2>The Alert Fatigue Problem</h2>
  <p>
    My first week as a DevOps engineer at Google, I received 47 alert emails. 
    Most were false alarms. By day three, I was ignoring all alerts. By day 
    five, a real issue slipped through because it looked like every other noisy alert.
  </p>

  <p>
    Bad monitoring is worse than no monitoring. It creates a "boy who cried wolf" 
    scenario where real issues get lost in the noise.
  </p>

  <h2>The Three-Alert Rule</h2>
  <p>
    Here's my controversial opinion: if you have more than three types of alerts, 
    you're probably doing it wrong. Focus on what actually matters:
  </p>

  <ol>
    <li><strong>Is it down?</strong> (Your service isn't responding)</li>
    <li><strong>Is it slow?</strong> (Response times are awful)</li>
    <li><strong>Is it broken?</strong> (Error rates are spiking)</li>
  </ol>

  <p>
    Everything else is nice to know, but shouldn't wake you up at 3 AM.
  </p>

  <h2>Start with the Golden Signals</h2>
  <p>
    Google's Site Reliability Engineering book talks about four golden signals. 
    I simplified it to three for my team, because we're busy humans, not robots.
  </p>

  <h3>Latency (Speed)</h3>
  <pre><code>// Simple latency monitoring in Go
func instrumentHandler(handler http.HandlerFunc) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        handler(w, r)
        duration := time.Since(start)
        
        // Alert if 95th percentile > 500ms
        if duration > 500*time.Millisecond {
            log.Printf("SLOW REQUEST: %s took %v", r.URL.Path, duration)
        }
    }
}</code></pre>

  <h3>Error Rate</h3>
  <pre><code>// Count errors vs. successful requests
var (
    totalRequests = 0
    errorRequests = 0
)

func trackRequest(statusCode int) {
    totalRequests++
    if statusCode >= 400 {
        errorRequests++
    }
    
    errorRate := float64(errorRequests) / float64(totalRequests)
    if errorRate > 0.05 { // 5% error rate threshold
        sendAlert("Error rate spike: %.2f%%", errorRate*100)
    }
}</code></pre>

  <h2>Dashboards for Humans</h2>
  <p>
    Most monitoring dashboards look like NASA mission control. They're impressive 
    but useless during an outage when you need answers fast.
  </p>

  <h3>My Dashboard Philosophy:</h3>
  <ul>
    <li><strong>Big numbers:</strong> Status should be visible from across the room</li>
    <li><strong>Traffic lights:</strong> Green = good, Yellow = watch, Red = fix now</li>
    <li><strong>Time ranges that matter:</strong> Last hour, last day, last week</li>
    <li><strong>One-click drilldown:</strong> Click to see what's actually broken</li>
  </ul>

  <blockquote>
    "If you need to squint at your dashboard to understand if things are working, 
    your dashboard needs work."
  </blockquote>

  <h2>Logs That Tell Stories</h2>
  <p>
    Stop logging everything. Start logging stories. When something breaks, you 
    want to understand what happened, not dig through 10,000 debug statements.
  </p>

  <h3>Good Logging Practice:</h3>
  <pre><code>// Instead of this:
log.Debug("Processing user data")
log.Debug("Validating input")
log.Debug("Calling database")
log.Debug("Returning response")

// Do this:
log.Info("Processing login request", 
    "user_id", userID, 
    "ip", clientIP, 
    "duration", duration)

log.Error("Login failed", 
    "user_id", userID, 
    "reason", "invalid_password", 
    "attempts_today", attemptCount)</code></pre>

  <h2>Health Checks That Actually Work</h2>
  <p>
    A health check that just returns "OK" is useless. Test the things that 
    actually matter for your service to work.
  </p>

  <pre><code>func healthCheck(w http.ResponseWriter, r *http.Request) {
    status := struct {
        Database string `json:"database"`
        Cache    string `json:"cache"`
        API      string `json:"external_api"`
        Overall  string `json:"status"`
    }{}
    
    // Check database connection
    if err := db.Ping(); err != nil {
        status.Database = "failing"
        status.Overall = "degraded"
    } else {
        status.Database = "healthy"
    }
    
    // Check cache
    if err := cache.Ping(); err != nil {
        status.Cache = "failing"
        // Cache failure might not be critical
        if status.Overall == "" {
            status.Overall = "degraded"
        }
    } else {
        status.Cache = "healthy"
    }
    
    if status.Overall == "" {
        status.Overall = "healthy"
    }
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(status)
}</code></pre>

  <h2>Alert Fatigue Solutions</h2>

  <h3>Smart Grouping</h3>
  <p>
    If five things break at once, send one alert, not five. Most cascading 
    failures have a root cause.
  </p>

  <h3>Escalation Paths</h3>
  <p>
    Not every alert needs to page the on-call engineer immediately. Try:
  </p>
  <ol>
    <li>Send to Slack channel (team sees it)</li>
    <li>If not acknowledged in 15 minutes, email on-call</li>
    <li>If not acknowledged in 30 minutes, call on-call</li>
  </ol>

  <h3>Time-Based Sensitivity</h3>
  <p>
    A 30-second outage at 3 PM is different from 3 AM. Adjust your thresholds 
    based on business impact.
  </p>

  <h2>Tools That Don't Suck</h2>
  <p>
    You don't need enterprise monitoring solutions. Start simple:
  </p>

  <ul>
    <li><strong>Uptime monitoring:</strong> UptimeRobot or Pingdom</li>
    <li><strong>Error tracking:</strong> Sentry (best free tier ever)</li>
    <li><strong>Basic metrics:</strong> Prometheus + Grafana</li>
    <li><strong>Log aggregation:</strong> Start with structured logging</li>
    <li><strong>Alerting:</strong> PagerDuty or Slack webhooks</li>
  </ul>

  <h2>The Human Side of Monitoring</h2>
  <p>
    Remember that alerts interrupt real humans with real lives. Design your 
    monitoring like you're designing a user interface â€“ optimize for the human 
    experience, not just technical correctness.
  </p>

  <h3>Good Alert Principles:</h3>
  <ul>
    <li>Every alert should be actionable</li>
    <li>Include context in the alert message</li>
    <li>Link to runbooks or debugging guides</li>
    <li>Set clear expectations for response time</li>
  </ul>

  <h2>The Kawaii DevOps Philosophy</h2>
  <p>
    Good monitoring should make your team feel confident and supported, not 
    stressed and overwhelmed. When alerts fire, the response should be "I know 
    exactly what to do" not "oh no, what's broken now?"
  </p>

  <p>
    My monitoring setup has cute names (our database health check is called 
    "db-chan") and friendly error messages. Why? Because 3 AM debugging is 
    stressful enough without hostile tooling.
  </p>

  <h2>Start Small, Iterate</h2>
  <p>
    Don't try to build the perfect monitoring system on day one. Start with 
    basic uptime checks and one dashboard. Add complexity only when you 
    understand what questions you're trying to answer.
  </p>

  <p>
    The best monitoring system is the one your team actually uses during 
    incidents. Keep it simple, keep it human, and keep iterating based on 
    real experiences.
  </p>

  <p>
    Your future 3 AM self will thank you for building monitoring that helps 
    instead of hinders. And maybe you'll even get to keep your kawaii pajamas 
    on while fixing things.
  </p>
</BlogLayout>
